{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f7bc713",
   "metadata": {},
   "source": [
    "# Produce table-entry-page-retrieval-effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000976c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install trectools\n",
    "\n",
    "from trectools import TrecQrel, TrecRun, TrecEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43fa5d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "QREL_DIR='../../Data/navigational-topics-and-qrels-ms-marco-v1/'\n",
    "QREL_DIR_MARCO_V2='../../Data/navigational-topics-and-qrels-ms-marco-v2/'\n",
    "\n",
    "RUN_DIR='/mnt/ceph/storage/data-in-progress/data-teaching/theses/wstud-thesis-probst/retrievalExperiments/runs-ecir22/'\n",
    "RUN_DIR_MARCO_V2='/mnt/ceph/storage/data-in-progress/data-teaching/theses/wstud-thesis-probst/retrievalExperiments/runs-marco-v2-ecir22/'\n",
    "\n",
    "TOPIC_TO_NAME={'entrypage-random': 'Random@V1', 'entrypage-popular': 'Popular@V1'}\n",
    "QRELS={i: TrecQrel(QREL_DIR + 'qrels.msmarco-' + i + '.txt') for i in TOPIC_TO_NAME.keys()}\n",
    "QRELS_MARCO_V2={i: TrecQrel(QREL_DIR_MARCO_V2 + 'qrels.msmarco-v2-' + i + '.txt') for i in TOPIC_TO_NAME.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15ebaaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPROACH_TO_MARCO_V1_RUN_FILE={\n",
    "    'BM25@2016-07': 'run.cc-16-07-anchortext.bm25-default.txt',\n",
    "    'BM25@2017-04': 'run.cc-17-04-anchortext.bm25-default.txt',\n",
    "    'BM25@2018-13': 'run.cc-18-13-anchortext.bm25-default.txt',\n",
    "    'BM25@2019-47': 'run.cc-19-47-anchortext.bm25-default.txt',\n",
    "    'BM25@2020-05': 'run.cc-20-05-anchortext.bm25-default.txt',\n",
    "    'BM25@2021-04': 'run.cc-21-04-anchortext.bm25-default.txt',\n",
    "    'BM25@16--21': 'run.cc-combined-anchortext.bm25-default.txt',\n",
    "    'BM25@Content': 'run.ms-marco-content.bm25-default.txt',\n",
    "    'BM25@Title': 'run.msmarco-document-v1-title-only.pos+docvectors+raw.bm25-default.txt',\n",
    "    'BM25@Orcas': 'run.orcas.bm25-default.txt',\n",
    "    'DeepCT@Anchor': 'run.ms-marco-deepct-v1-anserini-docs-cc-2019-47-sampled-test-overlap-removed-389979.bm25-default.txt',\n",
    "    'DeepCT@Orcas': 'run.ms-marco-deepct-v1-anserini-docs-orcas-sampled-test-overlap-removed-390009.bm25-default.txt',\n",
    "    'DeepCT@Train':'run.ms-marco-deepct-v1-anserini-docs-ms-marco-training-set-test-overlap-removed-389973.bm25-default.txt',\n",
    "    'MonoT5': 'run.ms-marco-content.bm25-mono-t5-maxp.txt',\n",
    "    'MonoBERT': 'run.ms-marco-content.bm25-mono-bert-maxp.txt',\n",
    "    'LambdaMART@CTA':'run.ms-marco.lambda-mart-cta-trees-1000.txt',\n",
    "    'LambdaMART@CTOA':'run.ms-marco.lambda-mart-ctoa-trees-1000.txt',\n",
    "    'LambdaMART@CTO':'run.ms-marco.lambda-mart-cto-trees-1000.txt',\n",
    "    'LambdaMART@CT':'run.ms-marco.lambda-mart-ct-trees-1000.txt',\n",
    "}\n",
    "\n",
    "APPROACH_TO_MARCO_V2_RUN_FILE={\n",
    "    'BM25@Content': 'run.msmarco-doc-v2.bm25-default.txt',\n",
    "    'BM25@Orcas': 'run.orcas-ms-marco-v2.bm25-default.txt',\n",
    "    'BM25@2016-07': 'run.cc-16-07-anchortext.bm25-default.txt',\n",
    "    'BM25@2017-04': 'run.cc-17-04-anchortext.bm25-default.txt',\n",
    "    'BM25@2018-13': 'run.cc-18-13-anchortext.bm25-default.txt',\n",
    "    'BM25@2019-47': 'run.cc-19-47-anchortext-v2.bm25-default.txt',\n",
    "    'BM25@2020-05': 'run.cc-20-05-anchortext.bm25-default.txt',\n",
    "    'BM25@2021-04': 'run.cc-21-04-anchortext.bm25-default.txt',\n",
    "    'BM25@16--21': 'run.cc-union-16-to-21-anchortext-1000.bm25-default.txt',\n",
    "    'DeepCT@Anchor': 'run.ms-marco-deepct-v2-anserini-docs-cc-2019-47-sampled-test-overlap-removed-389979.bm25-default.txt',\n",
    "    'DeepCT@Orcas': 'run.ms-marco-deepct-v2-anserini-docs-orcas-sampled-test-overlap-removed-390009.bm25-default.txt',\n",
    "    'DeepCT@Train':'run.ms-marco-deepct-v2-anserini-docs-ms-marco-training-set-test-overlap-removed-389973.bm25-default.txt',\n",
    "    'MonoT5': 'run.ms-marco-content.bm25-mono-t5-maxp.txt',\n",
    "    'MonoBERT': 'run.ms-marco-content.bm25-mono-bert-maxp.txt',\n",
    "    'LambdaMART@CTA':'run.ms-marco.lambda-mart-cta-trees-1000.txt',\n",
    "    'LambdaMART@CTOA':'run.ms-marco.lambda-mart-ctoa-trees-1000.txt',\n",
    "    'LambdaMART@CTO':'run.ms-marco.lambda-mart-cto-trees-1000.txt',\n",
    "    'LambdaMART@CT':'run.ms-marco.lambda-mart-ct-trees-1000.txt',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "019fe650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}[bt]\n",
      "    \\setlength{\\tabcolsep}{0.3em}\n",
      "    \\caption{Overview of the retrieval effectiveness on 100~random entry page topics and 100~entry page topics for popular pages on version~1 of MS~Marco (V1) and version~2 of MS~Marco (V2). We Report the mean reciprocal rank (MRR) and the recall at~3 (R@3) and at~10 (R@10).}\n",
      "    \\label{table-entry-page-retrieval-effectiveness}\n",
      "    \\scriptsize\n",
      "    \\begin{tabular*}{\\textwidth}{@{\\extracolsep{\\fill}}ll@{\\qquad}ccc@{\\quad}ccc@{\\quad}ccc@{\\quad}ccc@{}}\n",
      "\n",
      "        \\toprule\n",
      "\n",
      "        & & \\multicolumn{3}{@{}c@{\\quad}}{Random@V1} & \\multicolumn{3}{@{}c@{\\quad}}{Popular@V1} & \\multicolumn{3}{@{}c@{\\qquad}}{Random@V2} & \\multicolumn{3}{@{}c@{}}{Popular@V2} \\\\\n",
      "\n",
      "        \\cmidrule(r{1em}){3-5} \\cmidrule(r{1em}){6-8} \\cmidrule(r{1em}){9-11} \\cmidrule{12-14}\n",
      "\n",
      "        & & MRR & R@3 & R@10 & MRR & R@3 & R@10 & MRR & R@3 & R@10 & MRR & R@3 & R@10 \\\\\n",
      "\n",
      "        \\midrule\n",
      "\n",
      "        \\multirow{7}{*}{\\rotatebox[origin=c]{90}{\\parbox[c]{4em}{\\centering \\textbf{Anchor}}}}\n",
      "        & BM25@2016-07 & 0.61& 0.63& 0.68& {\\textbf{0.62}} & {\\textbf{0.72}} & 0.83&  0.56& 0.61& 0.64& {\\textbf{0.57}} & {\\textbf{0.64}} & {\\textbf{0.80}}  \\\\\n",
      "        \n",
      "        & BM25@2017-04 & 0.63& 0.70& 0.73& 0.59& 0.67& 0.84&  0.59& 0.68& 0.70& 0.48& 0.56& 0.73 \\\\\n",
      "\n",
      "        & BM25@2018-13 & 0.70& 0.76& 0.82& 0.54& 0.65& 0.81&  0.62& 0.68& 0.77& 0.47& 0.54& 0.77 \\\\\n",
      "\n",
      "        & BM25@2019-47 & 0.63& 0.74& 0.78& 0.58& 0.69& 0.84&  0.59& 0.62& 0.76& 0.49& 0.57& 0.78 \\\\\n",
      "\n",
      "        & BM25@2020-05 & 0.63& 0.72& 0.79& 0.55& 0.66& {\\textbf{0.86}} &  0.56& 0.64& 0.71& 0.45& 0.53& 0.74 \\\\\n",
      "\n",
      "        & BM25@2021-04 & 0.63& 0.73& 0.77& 0.54& 0.66& 0.80&  0.50& 0.54& 0.64& 0.46& 0.55& 0.73 \\\\\n",
      "        \n",
      "        & BM25@16--21 & {\\textbf{0.74}} & {\\textbf{0.83}} & {\\textbf{0.89}} & 0.55& 0.66& 0.84&  {\\textbf{0.67}} & {\\textbf{0.73}} & {\\textbf{0.85}} & 0.39& 0.48& 0.70 \\\\\n",
      "\n",
      "        \\midrule\n",
      "\n",
      "        \\multirow{7}{*}{\\rotatebox[origin=c]{90}{\\parbox[c]{4em}{\\centering \\textbf{Baselines}}}}\n",
      "        & BM25@Content & 0.21& 0.24& 0.36& 0.02& 0.02& 0.03&  0.21& 0.22& 0.42& 0.02& 0.01& 0.04 \\\\\n",
      "\n",
      "        & BM25@Orcas & {\\textbf{0.60}} & {\\textbf{0.64}} & {\\textbf{0.70}} & {\\textbf{0.28}} & {\\textbf{0.32}} & {\\textbf{0.43}} &  {\\textbf{0.56}} & {\\textbf{0.59}} & {\\textbf{0.66}} & {\\textbf{0.28}} & {\\textbf{0.33}} & {\\textbf{0.44}}  \\\\\n",
      "        \n",
      "        & DeepCT@Anchor & 0.43& 0.46& 0.58& 0.03& 0.03& 0.08&  0.43& 0.49& {\\textbf{0.66}} & 0.04& 0.03& 0.13 \\\\\n",
      "\n",
      "        & DeepCT@Orcas & 0.38& 0.42& 0.57& 0.02& 0.00& 0.09&  0.36& 0.40& 0.60& 0.05& 0.04& 0.10 \\\\\n",
      "\n",
      "        & DeepCT@Train & 0.27& 0.28& 0.44& 0.02& 0.01& 0.05&  0.32& 0.34& 0.49& 0.03& 0.02& 0.08 \\\\\n",
      "        \n",
      "        & MonoT5 & 0.39& 0.43& 0.53& 0.02& 0.01& 0.05&  0.38& 0.43& 0.57& 0.04& 0.04& 0.08 \\\\\n",
      "        \n",
      "        & MonoBERT & 0.35& 0.37& 0.51& 0.02& 0.01& 0.05&  0.36& 0.41& 0.56& 0.01& 0.01& 0.02 \\\\\n",
      "        \n",
      "        & LambdaMART@CTOA & 0.48& 0.55& 0.63& 0.08& 0.07& 0.18&  0.52& 0.57& {\\textbf{0.79}} & 0.12& 0.12& 0.21 \\\\\n",
      "        \n",
      "        & LambdaMART@CTO & 0.41& 0.49& 0.57& 0.07& 0.06& 0.17&  0.49& 0.55& 0.65& 0.08& 0.10& 0.14 \\\\\n",
      "        \n",
      "        & LambdaMART@CTA & 0.43& 0.51& 0.61& 0.06& 0.06& 0.19&  0.55& {\\textbf{0.62}} & {\\textbf{0.77}} & 0.14& 0.15& 0.24 \\\\\n",
      "        \n",
      "        & LambdaMART@CT & 0.27& 0.31& 0.46& 0.04& 0.03& 0.09&  0.40& 0.44& 0.60& 0.05& 0.05& 0.08 \\\\\n",
      "        \n",
      "        \\bottomrule\n",
      "\n",
      "    \\end{tabular*}\n",
      "    \\vspace*{-2ex}\n",
      "\\end{table*} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def recall(trec_eval, depth):\n",
    "    import pandas as pd\n",
    "    trecformat = trec_eval.run.run_data.sort_values([\"query\", \"score\", \"docid\"], ascending=[True,False,False]).reset_index()\n",
    "    topX = trecformat.groupby(\"query\")[[\"query\",\"docid\"]].head(depth)\n",
    "    merged = pd.merge(topX[[\"query\",\"docid\"]], trec_eval.qrels.qrels_data[[\"query\",\"docid\",\"rel\"]])\n",
    "\n",
    "    nqueries = len(trec_eval.qrels.topics())\n",
    "    result = merged[merged[\"rel\"]>0].groupby(\"query\")[\"rel\"].count()\n",
    "    \n",
    "    return result.sum()/nqueries\n",
    "\n",
    "def is_anchor_text(run_file):\n",
    "    return '-anchortext' in run_file\n",
    "\n",
    "def format_score(score, run_file, topics, position):\n",
    "    # Manually maintained\n",
    "    if is_anchor_text(run_file):\n",
    "        max_scores = {\n",
    "            'entrypage-random': [0.74, 0.82, 0.88, 0.66, 0.69, 0.8],\n",
    "            'entrypage-popular': [0.62, 0.71, 0.85, 0.56, 0.63, 0.79],\n",
    "        }\n",
    "    else:\n",
    "        max_scores =  {\n",
    "            'entrypage-random': [0.59, 0.63, 0.69, 0.55, 0.58, 0.65],\n",
    "            'entrypage-popular': [0.28, 0.32, 0.43, 0.27, 0.32, 0.43],\n",
    "        }\n",
    "        \n",
    "    ret = '{:.2f}'.format(score)\n",
    "    if max_scores[topics][position] <= score:\n",
    "        return '{\\\\textbf{' + ret + '}} '\n",
    "    else:\n",
    "        return ret\n",
    "\n",
    "def eval_on_marco_v1(run_file):\n",
    "    ret = ' '\n",
    "    for topics in ['entrypage-random', 'entrypage-popular']:\n",
    "        run = TrecRun(RUN_DIR + topics + '/' + run_file)\n",
    "        trec_eval=TrecEval(run, QRELS[topics])\n",
    "        ret += '& ' + format_score(trec_eval.get_reciprocal_rank(), run_file, topics, 0)\n",
    "        ret += '& ' + format_score(recall(trec_eval, 3), run_file, topics, 1)\n",
    "        ret += '& ' + format_score(recall(trec_eval, 10), run_file, topics, 2)\n",
    "    \n",
    "    return ret + '&'\n",
    "\n",
    "def eval_on_marco_v2(run_file):\n",
    "    ret = ' '\n",
    "    for topics in ['entrypage-random', 'entrypage-popular']:\n",
    "        run = TrecRun(RUN_DIR_MARCO_V2 + topics + '/' + run_file)\n",
    "        trec_eval=TrecEval(run, QRELS_MARCO_V2[topics])\n",
    "        ret += (' ' if 'random' in topics else '& ') + format_score(trec_eval.get_reciprocal_rank(), run_file, topics, 3)\n",
    "        ret += '& ' + format_score(recall(trec_eval, 3), run_file, topics, 4)\n",
    "        ret += '& ' + format_score(recall(trec_eval, 10), run_file, topics, 5)\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def table_row(approach):\n",
    "    v1_eval = ' & --- & --- & --- & --- & --- & --- &'\n",
    "    \n",
    "    if approach in APPROACH_TO_MARCO_V1_RUN_FILE:\n",
    "        v1_eval = eval_on_marco_v1(APPROACH_TO_MARCO_V1_RUN_FILE[approach])\n",
    "    \n",
    "    v2_eval = ' --- & --- & --- & --- & --- & ---'\n",
    "    \n",
    "    if approach in APPROACH_TO_MARCO_V2_RUN_FILE:\n",
    "        v2_eval = eval_on_marco_v2(APPROACH_TO_MARCO_V2_RUN_FILE[approach])\n",
    "    \n",
    "    return '& ' + approach + v1_eval + v2_eval + ' \\\\\\\\'\n",
    "\n",
    "\n",
    "def table_entry_page_retrieval_effectiveness():\n",
    "    return '''\\\\begin{table*}[bt]\n",
    "    \\\\setlength{\\\\tabcolsep}{0.3em}\n",
    "    \\\\caption{Overview of the retrieval effectiveness on 100~random entry page topics and 100~entry page topics for popular pages on version~1 of MS~Marco (V1) and version~2 of MS~Marco (V2). We Report the mean reciprocal rank (MRR) and the recall at~3 (R@3) and at~10 (R@10).}\n",
    "    \\\\label{table-entry-page-retrieval-effectiveness}\n",
    "    \\\\scriptsize\n",
    "    \\\\begin{tabular*}{\\\\textwidth}{@{\\\\extracolsep{\\\\fill}}ll@{\\\\qquad}ccc@{\\\\quad}ccc@{\\\\quad}ccc@{\\\\quad}ccc@{}}\n",
    "\n",
    "        \\\\toprule\n",
    "\n",
    "        & & \\\\multicolumn{3}{@{}c@{\\\\quad}}{Random@V1} & \\\\multicolumn{3}{@{}c@{\\\\quad}}{Popular@V1} & \\\\multicolumn{3}{@{}c@{\\\\qquad}}{Random@V2} & \\\\multicolumn{3}{@{}c@{}}{Popular@V2} \\\\\\\\\n",
    "\n",
    "        \\\\cmidrule(r{1em}){3-5} \\\\cmidrule(r{1em}){6-8} \\\\cmidrule(r{1em}){9-11} \\\\cmidrule{12-14}\n",
    "\n",
    "        & & MRR & R@3 & R@10 & MRR & R@3 & R@10 & MRR & R@3 & R@10 & MRR & R@3 & R@10 \\\\\\\\\n",
    "\n",
    "        \\\\midrule\n",
    "\n",
    "        \\\\multirow{7}{*}{\\\\rotatebox[origin=c]{90}{\\\\parbox[c]{4em}{\\\\centering \\\\textbf{Anchor}}}}\n",
    "        ''' + table_row('BM25@2016-07') + '''\n",
    "        \n",
    "        ''' + table_row('BM25@2017-04') + '''\n",
    "\n",
    "        ''' + table_row('BM25@2018-13') + '''\n",
    "\n",
    "        ''' + table_row('BM25@2019-47') + '''\n",
    "\n",
    "        ''' + table_row('BM25@2020-05') + '''\n",
    "\n",
    "        ''' + table_row('BM25@2021-04') + '''\n",
    "        \n",
    "        ''' + table_row('BM25@16--21') + '''\n",
    "\n",
    "        \\\\midrule\n",
    "\n",
    "        \\\\multirow{7}{*}{\\\\rotatebox[origin=c]{90}{\\\\parbox[c]{4em}{\\\\centering \\\\textbf{Baselines}}}}\n",
    "        ''' + table_row('BM25@Content') + '''\n",
    "\n",
    "        ''' + table_row('BM25@Orcas') + '''\n",
    "        \n",
    "        ''' + table_row('DeepCT@Anchor') + '''\n",
    "\n",
    "        ''' + table_row('DeepCT@Orcas') + '''\n",
    "\n",
    "        ''' + table_row('DeepCT@Train') + '''\n",
    "        \n",
    "        ''' + table_row('MonoT5') + '''\n",
    "        \n",
    "        ''' + table_row('MonoBERT') + '''\n",
    "        \n",
    "        ''' + table_row('LambdaMART@CTOA') + '''\n",
    "        \n",
    "        ''' + table_row('LambdaMART@CTO') + '''\n",
    "        \n",
    "        ''' + table_row('LambdaMART@CTA') + '''\n",
    "        \n",
    "        ''' + table_row('LambdaMART@CT') + '''\n",
    "        \n",
    "        \\\\bottomrule\n",
    "\n",
    "    \\\\end{tabular*}\n",
    "    \\\\vspace*{-2ex}\n",
    "\\\\end{table*} \n",
    "'''\n",
    "\n",
    "print(table_entry_page_retrieval_effectiveness())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
