{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d5ce4c6-4657-441c-b7c7-7d66f1fd3662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data for DeppCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b031396-ce3f-4909-b95a-064634f52f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.23.76.114:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c71f903-d885-4bf1-acee-4022b22f6583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_queries(i):\n",
    "    import json\n",
    "    i = json.loads(i)\n",
    "    \n",
    "    return (i['targetDocumentId'], [j['anchorText'] for j in i['anchorTextSample']])\n",
    "\n",
    "def to_json(i):\n",
    "    import json\n",
    "    \n",
    "    anchorText = []\n",
    "    for j in i[1]:\n",
    "        anchorText += j\n",
    "    \n",
    "    return json.dumps({\n",
    "        'targetDocumentId': i[0],\n",
    "        'anchorText': anchorText\n",
    "    })\n",
    "\n",
    "sc.textFile('ecir22-filtered-commoncrawl-2019-47-sample/*/*')\\\n",
    "    .map(map_to_queries)\\\n",
    "    .groupByKey()\\\n",
    "    .map(to_json)\\\n",
    "    .repartition(10)\\\n",
    "    .saveAsTextFile('ecir22-filtered-commoncrawl-2019-47-deepct-sample.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef6e8557-9881-4b2b-a2aa-15986315bb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fab390d0-d58e-4399-9c8c-79f87898f9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.6 MB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /home/jovyan/.local/lib/python3.7/site-packages (from en-core-web-sm==3.1.0) (3.1.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/jovyan/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/jovyan/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/jovyan/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/jovyan/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/jovyan/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /home/jovyan/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.25.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /home/jovyan/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /home/jovyan/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/jovyan/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/jovyan/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.61.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/jovyan/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.20.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.1)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/jovyan/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (20.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/jovyan/.local/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2021.5.30)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72c2c57e-4e24-46ee-a578-2269fa29aa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['senter']\n"
     ]
    }
   ],
   "source": [
    "# From https://github.com/grill-lab/trec-cast-tools/blob/master/src/main/python/passage_chunker.py\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\", \"tagger\", \"ner\", \"attribute_ruler\", \"lemmatizer\", \"tok2vec\"]) #->senter\n",
    "nlp.enable_pipe(\"senter\")\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "\n",
    "class SpacyPassageChunker:\n",
    "    def __init__(self):\n",
    "        self.document = None\n",
    "        self.sentences = None\n",
    "\n",
    "    \n",
    "    def sentence_tokenization(self, document):\n",
    "        self.document = parse_doc(document)\n",
    "        self.sentences = list(self.document.sents)\n",
    "\n",
    "    \n",
    "    def create_passages(self, passage_size = 250):\n",
    "\n",
    "        passages = []\n",
    "        content_length = len(self.sentences)\n",
    "        sentences_word_count = [len([token for token in sentence]) for sentence in self.sentences]\n",
    "        \n",
    "        current_idx = 0\n",
    "        current_passage_word_count = 0\n",
    "        current_passage = ''\n",
    "        sub_id = 0\n",
    "        \n",
    "        for i in range(content_length):\n",
    "            if current_passage_word_count >= (passage_size * 0.67):\n",
    "                passages.append({\n",
    "                    \"body\": current_passage,\n",
    "                    \"id\": sub_id\n",
    "                })\n",
    "                current_passage = ''\n",
    "                current_passage_word_count = 0\n",
    "                \n",
    "                current_idx = i\n",
    "                sub_id += 1\n",
    "            \n",
    "            current_passage += self.sentences[i].text + ' '\n",
    "            current_passage_word_count += sentences_word_count[i]\n",
    "\n",
    "        \n",
    "        current_passage = ' '.join(sentence.text for sentence in self.sentences[current_idx:])\n",
    "        passages.append({\n",
    "            \"body\": current_passage,\n",
    "            \"id\": sub_id\n",
    "        })\n",
    "        \n",
    "        return passages\n",
    "        \n",
    "        \n",
    "def parse_doc(document):\n",
    "    return nlp(sanitize_document(document))\n",
    "\n",
    "def sanitize_document(doc):\n",
    "    sanitized = re.compile('<.*?>')\n",
    "    return re.sub(sanitized, '', doc)\n",
    "\n",
    "def tokens(doc):\n",
    "    return set([i for i in parse_doc(doc) if not i.is_punct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3737a922-ae81-46fd-b8a5-258b16058ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4, Kid, Speed, Reading, Toenail, Now, Fungus, Kill}\n"
     ]
    }
   ],
   "source": [
    "print(tokens('Speed Reading 4 Kid: Kill Toenail Fungus Now!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c39b84d7-184f-4563-aff5-1bffb6a7a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "\n",
    "ms_marco_docs = ir_datasets.load('msmarco-document').docs_store()\n",
    "\n",
    "def passages(doc):\n",
    "    ret = [doc.title]\n",
    "    tmp = SpacyPassageChunker()\n",
    "    tmp.sentence_tokenization(doc.body)\n",
    "    \n",
    "    ret += [i['body'] for i in tmp.create_passages(80)]\n",
    "\n",
    "    return [(i[0] +1, i[1]) for i in enumerate(ret)]\n",
    "\n",
    "#passages('D1112106')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79d5e47e-c870-4713-b9d2-bd065d5f2a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1163273it [00:48, 24186.80it/s]\n"
     ]
    }
   ],
   "source": [
    "ids = []\n",
    "\n",
    "with open(DIR + 'cc-2019-47-anchors.jsonl', 'r') as f:\n",
    "    for l in tqdm(f):\n",
    "        ids += [json.loads(l)['targetDocumentId']]\n",
    "ids = set(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd49506a-3c38-4aaa-85df-92bda88ad1a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-31de58711e9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mms_marco_docs_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mms_marco_doc_to_passages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         '''\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_map_async\u001b[0;34m(self, func, iterable, mapper, chunksize, callback, error_callback)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pool not running\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0miterable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-31de58711e9c>\u001b[0m in \u001b[0;36mms_marco_docs_iter\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mir_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'msmarco-document'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ids' is not defined"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "pool = multiprocessing.Pool(processes=20)\n",
    "\n",
    "def ms_marco_docs_iter():\n",
    "    import ir_datasets\n",
    "    iter = ir_datasets.load('msmarco-document').docs_iter()\n",
    "    for doc in iter:\n",
    "        if doc.doc_id in ids:\n",
    "            yield doc\n",
    "\n",
    "tmp = pool.map(lambda j: (j.doc_id, passages(j)), ms_marco_docs_iter())\n",
    "ms_marco_doc_to_passages = {i[0]: i[1] for i in tqdm(tmp)}\n",
    "\n",
    "ms_marco_doc_to_passages['D1112106']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd103d3-b2b2-4e4b-8048-fa96c5a49a61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ms_marco_doc_to_passages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4a4c42121d01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mms_marco_doc_to_passages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'D1112106'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ms_marco_doc_to_passages' is not defined"
     ]
    }
   ],
   "source": [
    "ms_marco_doc_to_passages['D1112106']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d126f8ca-dadb-46b4-bf3d-be73e2f9453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ms_marco_doc_to_passages_processed = pool.map(ms_marco_doc_to_passages.values(), lambda i: (i.doc_id, passages(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8fdddff-7d7f-40dd-830d-c8800486d5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'body'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cb2c7143529b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'anchorText'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpassage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpassages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targetDocumentId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'doc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpassage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'queries'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-972ca732ec86>\u001b[0m in \u001b[0;36mpassages\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpacyPassageChunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_tokenization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_passages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'body'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "DIR = '/mnt/ceph/storage/data-in-progress/data-teaching/theses/wstud-thesis-probst/deep-ct/'\n",
    "\n",
    "with open(DIR + 'cc-2019-47-anchors.jsonl', 'r') as f, open(DIR + 'deep-ct-training-data.jsonl', 'w') as out:\n",
    "    for l in tqdm(f):\n",
    "        l = json.loads(l)\n",
    "        anchors = l['anchorText']\n",
    "        \n",
    "        for passage in passages(l['targetDocumentId']):\n",
    "            out.write(json.dumps({'doc': {'title': passage}, 'queries': anchors}) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
